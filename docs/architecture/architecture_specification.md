# ğŸ¸ Model Architecture Specification
## Guitar Chord & Strumming Pattern Generator

**Document Version:** 1.0  
**Chat:** 6 - Model Design (Architecture Only)  
**Date:** January 2026  
**Author:** Rohan Rajendra Dhanawade  
**Purpose:** Technical specification for Chat 7 implementation

---

## 1. Executive Summary

This document specifies the neural architecture for generating symbolic guitar chord progressions and strumming patterns from natural language prompts. Two sequence models will be implemented and compared:

1. **LSTM Model** (Primary) â€” Simpler, more stable for small datasets
2. **Transformer Model** (Comparison) â€” Modern architecture for ablation study

Both models share the same tokenizer, feature encoder, and training pipeline.

---

## 2. System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           COMPLETE SYSTEM PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    USER PROMPT: "upbeat folk in G major"                                    â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚    â”‚         DISTILBERT PARSER               â”‚  â—„â”€â”€ Already built (Chat 5) â”‚
â”‚    â”‚     (Neural Prompt Feature Extractor)   â”‚                             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚              â”‚  PROMPT FEATURES  â”‚                                         â”‚
â”‚              â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                         â”‚
â”‚              â”‚  key: "G"         â”‚                                         â”‚
â”‚              â”‚  mode: "major"    â”‚                                         â”‚
â”‚              â”‚  genre: "folk"    â”‚                                         â”‚
â”‚              â”‚  emotion: "upbeat"â”‚                                         â”‚
â”‚              â”‚  tempo: 110       â”‚                                         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚    â”‚         FEATURE ENCODER                 â”‚  â—„â”€â”€ NEW (Chat 7)           â”‚
â”‚    â”‚   (Embeddings â†’ Conditioning Vector)    â”‚                             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚              â”‚ CONDITIONING      â”‚                                         â”‚
â”‚              â”‚ VECTOR (128 dims) â”‚                                         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                         â”‚                                                   â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚          â–¼                             â–¼                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚    â”‚   LSTM    â”‚                 â”‚TRANSFORMERâ”‚  â—„â”€â”€ NEW (Chat 7)           â”‚
â”‚    â”‚  MODEL    â”‚                 â”‚   MODEL   â”‚                             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚          â”‚                             â”‚                                   â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚    â”‚            TOKENIZER                    â”‚  â—„â”€â”€ NEW (Chat 7)           â”‚
â”‚    â”‚      (Decode token IDs â†’ text)          â”‚                             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚    â”‚           VALIDATOR                     â”‚  â—„â”€â”€ Already built (Chat 5) â”‚
â”‚    â”‚   (Check harmonic correctness)          â”‚                             â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                         â”‚                                                   â”‚
â”‚                         â–¼                                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚              â”‚   FINAL OUTPUT    â”‚                                         â”‚
â”‚              â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                         â”‚
â”‚              â”‚  chords: [G,D,Em,C]â”‚                                        â”‚
â”‚              â”‚  strum: "D_DU_DU_"â”‚                                         â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Tokenization Specification

### 3.1 Vocabulary Design Decisions

| Element | Tokenization Strategy | Rationale |
|---------|----------------------|-----------|
| Chords | One token per chord | Chords are atomic musical units |
| Strumming | Character-by-character | D, U, _ are atomic rhythmic events |
| Sequence | Combined with `<SEP>` | Single model, simpler training |

### 3.2 Complete Vocabulary

```python
VOCABULARY = {
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SPECIAL TOKENS (IDs 0-3)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "<PAD>": 0,   # Padding token (fills sequences to equal length)
    "<BOS>": 1,   # Beginning of sequence
    "<EOS>": 2,   # End of sequence  
    "<SEP>": 3,   # Separator between chords and strumming
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CHORD TOKENS (IDs 4-32) â€” 29 unique chords from dataset
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Natural major chords
    "C": 4, "D": 5, "E": 6, "F": 7, "G": 8, "A": 9, "B": 10,
    
    # Sharp major chords
    "A#": 11, "C#": 12, "D#": 13, "G#": 14,
    
    # Minor chords
    "Am": 15, "Bm": 16, "Cm": 17, "Dm": 18, "Em": 19, "Fm": 20, "Gm": 21,
    "A#m": 22, "C#m": 23, "F#m": 24, "G#m": 25,
    
    # Seventh chords
    "A7": 26, "B7": 27, "D7": 28, "E7": 29,
    
    # Other chord types
    "Asus4": 30, "C#dim": 31, "Gdim": 32,
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STRUMMING TOKENS (IDs 33-35) â€” 3 unique characters
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    "D": 33,  # Downstroke
    "U": 34,  # Upstroke
    "_": 35,  # Rest (muted/silent)
}

VOCAB_SIZE = 36  # Total vocabulary size
```

### 3.3 Sequence Format

**Example:** Folk song in G major, upbeat

```
Raw output:
  chords: ["G", "D", "Em", "C"]
  strum:  "D_DU_DU_"

Tokenized sequence:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚<BOS>â”‚ G â”‚ D â”‚ Em â”‚ C â”‚<SEP>â”‚ D â”‚ _ â”‚ D â”‚ U â”‚ _ â”‚ D â”‚ U â”‚ _ â”‚<EOS>â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
â”‚  1  â”‚ 8 â”‚ 5 â”‚ 19 â”‚ 4 â”‚  3  â”‚33 â”‚35 â”‚33 â”‚34 â”‚35 â”‚33 â”‚34 â”‚35 â”‚  2  â”‚

Sequence length: 15 tokens (typical)
Max sequence length: 20 tokens (with padding)
  - 1 <BOS> + 8 chords (max) + 1 <SEP> + 8 strum + 1 <EOS> + 1 buffer = 20
```

### 3.4 Important Note: Token ID Collision

âš ï¸ **The chord "D" and strumming "D" use DIFFERENT token IDs!**

| Token | Context | ID | Meaning |
|-------|---------|-----|---------|
| D | Before `<SEP>` | 5 | D major chord |
| D | After `<SEP>` | 33 | Downstroke |

The model learns to distinguish them by position (before vs. after `<SEP>`).

---

## 4. Feature Encoding Specification

### 4.1 Input Features (from DistilBERT Parser)

| Feature | Type | Possible Values | Count |
|---------|------|-----------------|-------|
| key | categorical | A, Am, Bm, C, D, Dm, E, Em, F, Fm, G, Gm | 12 |
| mode | categorical | major, minor | 2 |
| genre | categorical | acoustic, ballad, blues, country, folk, indie, jazz, pop, rock | 9 |
| emotion | categorical | dramatic, energetic, hopeful, melancholic, mellow, nostalgic, peaceful, upbeat | 8 |
| tempo | numerical | 40-200 BPM | continuous |

### 4.2 Embedding Dimensions

```python
EMBEDDING_CONFIG = {
    "key": {
        "num_values": 12,
        "embedding_dim": 32
    },
    "mode": {
        "num_values": 2,
        "embedding_dim": 16
    },
    "genre": {
        "num_values": 9,
        "embedding_dim": 32
    },
    "emotion": {
        "num_values": 8,
        "embedding_dim": 32
    },
    "tempo": {
        "num_buckets": 10,  # Bucketized: [40-55], [56-70], ..., [186-200]
        "embedding_dim": 16
    }
}

TOTAL_CONDITIONING_DIM = 32 + 16 + 32 + 32 + 16 = 128
```

### 4.3 Tempo Bucketization

```python
TEMPO_BUCKETS = [
    (40, 55),    # Bucket 0: Very slow
    (56, 70),    # Bucket 1: Slow
    (71, 85),    # Bucket 2: Slow-moderate
    (86, 100),   # Bucket 3: Moderate
    (101, 115),  # Bucket 4: Moderate-fast
    (116, 130),  # Bucket 5: Fast
    (131, 145),  # Bucket 6: Fast-energetic
    (146, 160),  # Bucket 7: Very fast
    (161, 180),  # Bucket 8: Driving
    (181, 200),  # Bucket 9: Maximum energy
]
```

### 4.4 Feature Encoder Architecture

```
INPUT FEATURES                    EMBEDDINGS                         OUTPUT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”€â”€â”€â”€â”€â”€

key: "G" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”œâ”€â–¶â”‚ Key Embedding      â”‚â”€â”€â–¶ [32 dims] â”€â”
                     â”‚ (12 Ã— 32 lookup)   â”‚               â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                                          â”‚
mode: "major" â”€â”€â”€â”€â”                                       â”‚
                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                  â”œâ”€â–¶â”‚ Mode Embedding     â”‚â”€â”€â–¶ [16 dims] â”€â”¤
                     â”‚ (2 Ã— 16 lookup)    â”‚               â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                                          â”‚
genre: "folk" â”€â”€â”€â”€â”                                       â”œâ”€â”€â–¶ CONCATENATE
                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚         â”‚
                  â”œâ”€â–¶â”‚ Genre Embedding    â”‚â”€â”€â–¶ [32 dims] â”€â”¤         â”‚
                     â”‚ (9 Ã— 32 lookup)    â”‚               â”‚         â–¼
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                          â”‚    â”‚ Linear  â”‚
emotion: "upbeat"â”€â”                                       â”‚    â”‚ 128â†’128 â”‚
                  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚ + ReLU  â”‚
                  â”œâ”€â–¶â”‚ Emotion Embedding  â”‚â”€â”€â–¶ [32 dims] â”€â”¤    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                     â”‚ (8 Ã— 32 lookup)    â”‚               â”‚         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚         â–¼
                                                          â”‚   CONDITIONING
tempo: 110 â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚     VECTOR
      â”‚           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚   [128 dims]
      â”‚           â”‚  â”‚ Bucketize          â”‚               â”‚
      â–¼           â”œâ”€â–¶â”‚ (110 â†’ bucket 4)   â”‚               â”‚
   bucket 4          â”‚                    â”‚               â”‚
                     â”‚ Tempo Embedding    â”‚â”€â”€â–¶ [16 dims] â”€â”˜
                     â”‚ (10 Ã— 16 lookup)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. LSTM Model Architecture

### 5.1 Architecture Diagram

```
                    CONDITIONING VECTOR (128 dims)
                              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                   â”‚
            â–¼                                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
    â”‚ Linear Layer  â”‚                           â”‚
    â”‚  (128 â†’ 256)  â”‚                           â”‚
    â”‚    + tanh     â”‚                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
            â”‚                                   â”‚
            â–¼                                   â”‚
    Initial Hidden State (hâ‚€)                   â”‚
    [1, batch, 256]                             â”‚
            â”‚                                   â”‚
            â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚     â”‚                             â”‚
            â–¼     â–¼                             â”‚
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚
    â•‘           LSTM LAYER                  â•‘   â”‚
    â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â•‘   â”‚
    â•‘  input_size: 64 + 128 = 192           â•‘   â”‚
    â•‘  hidden_size: 256                     â•‘   â”‚
    â•‘  num_layers: 2                        â•‘   â”‚
    â•‘  dropout: 0.2                         â•‘   â”‚
    â•‘  batch_first: True                    â•‘   â”‚
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
            â”‚                                   â”‚
            â”‚  At each timestep:               â”‚
            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
            â”‚  â”‚ input = [token_embed]    â”‚     â”‚
            â”‚  â”‚       + [conditioning]   â”‚â—„â”€â”€â”€â”€â”˜ (concatenated at every step)
            â”‚  â”‚       = [64 + 128]       â”‚
            â”‚  â”‚       = [192 dims]       â”‚
            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Output Layer  â”‚
    â”‚  (256 â†’ 36)   â”‚  â—„â”€â”€ 36 = vocab size
    â”‚   + softmax   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    Token Probabilities
    [batch, seq_len, 36]
```

### 5.2 LSTM Hyperparameters

```python
LSTM_CONFIG = {
    # Token embedding
    "vocab_size": 36,
    "token_embedding_dim": 64,
    
    # Conditioning
    "conditioning_dim": 128,
    
    # LSTM architecture
    "lstm_input_size": 64 + 128,  # token_embed + conditioning = 192
    "lstm_hidden_size": 256,
    "lstm_num_layers": 2,
    "lstm_dropout": 0.2,
    
    # Output
    "output_size": 36,  # vocab_size
    
    # Sequence
    "max_seq_length": 20,
}
```

### 5.3 LSTM Forward Pass (Pseudocode)

```python
def forward(self, prompt_features, target_sequence=None):
    """
    Args:
        prompt_features: dict with keys: key, mode, genre, emotion, tempo
        target_sequence: [batch, seq_len] - for teacher forcing during training
    
    Returns:
        logits: [batch, seq_len, vocab_size]
    """
    # 1. Encode features â†’ conditioning vector
    conditioning = self.feature_encoder(prompt_features)  # [batch, 128]
    
    # 2. Initialize hidden state from conditioning
    h0 = self.cond_to_hidden(conditioning)  # [batch, 256]
    h0 = h0.unsqueeze(0).repeat(2, 1, 1)    # [2, batch, 256] for 2 layers
    c0 = torch.zeros_like(h0)               # Cell state starts at zero
    
    # 3. Autoregressive generation
    if target_sequence is not None:
        # TRAINING: Teacher forcing
        token_embeds = self.token_embedding(target_sequence)  # [batch, seq, 64]
        
        # Concatenate conditioning at every step
        conditioning_expanded = conditioning.unsqueeze(1).expand(-1, seq_len, -1)
        lstm_input = torch.cat([token_embeds, conditioning_expanded], dim=-1)
        
        lstm_output, _ = self.lstm(lstm_input, (h0, c0))
        logits = self.output_layer(lstm_output)
    else:
        # INFERENCE: Generate one token at a time
        logits = self.generate_autoregressive(conditioning, h0, c0)
    
    return logits
```

---

## 6. Transformer Model Architecture

### 6.1 Key Difference: Conditioning via Prefix Tokens

Instead of hidden state initialization, the Transformer uses **prefix tokens**:

```
LSTM approach:
  conditioning â†’ hidden state hâ‚€
  
Transformer approach:
  conditioning â†’ special prefix tokens prepended to sequence
  
Sequence with prefix:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚<FOLK>â”‚<UPBEAT>â”‚<G_MAJ>â”‚<TEMPO_110>â”‚<BOS>â”‚ G â”‚ D â”‚ Em â”‚ C â”‚<SEP>â”‚ D _ D U ... â”‚<EOS>â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
   â†‘        â†‘        â†‘         â†‘
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         CONDITIONING PREFIX
         (model attends to these)
```

### 6.2 Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        TRANSFORMER DECODER-ONLY                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Input: [<FOLK>, <UPBEAT>, <G_MAJ>, <TEMPO_110>, <BOS>, G, D, Em, ...]    â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚                    TOKEN EMBEDDING                                â”‚     â”‚
â”‚   â”‚                  (vocab_size Ã— 64)                                â”‚     â”‚
â”‚   â”‚                                                                   â”‚     â”‚
â”‚   â”‚   Note: Conditioning features get their OWN embedding tokens:    â”‚     â”‚
â”‚   â”‚   <FOLK> = token 37, <UPBEAT> = token 45, etc.                   â”‚     â”‚
â”‚   â”‚   (extends vocabulary from 36 to ~65)                            â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚                POSITIONAL ENCODING                                â”‚     â”‚
â”‚   â”‚              (sinusoidal or learned)                              â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—     â”‚
â”‚   â•‘              TRANSFORMER DECODER BLOCK (Ã—4)                       â•‘     â”‚
â”‚   â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘     â”‚
â”‚   â•‘  â”‚            MASKED SELF-ATTENTION                           â”‚  â•‘     â”‚
â”‚   â•‘  â”‚  â€¢ 4 attention heads                                       â”‚  â•‘     â”‚
â”‚   â•‘  â”‚  â€¢ Each position attends to previous positions only        â”‚  â•‘     â”‚
â”‚   â•‘  â”‚  â€¢ Conditioning tokens are ALWAYS visible                  â”‚  â•‘     â”‚
â”‚   â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘     â”‚
â”‚   â•‘                              â”‚                                    â•‘     â”‚
â”‚   â•‘                              â–¼                                    â•‘     â”‚
â”‚   â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â•‘     â”‚
â”‚   â•‘  â”‚            FEED-FORWARD NETWORK                            â”‚  â•‘     â”‚
â”‚   â•‘  â”‚  Linear(256 â†’ 512) â†’ ReLU â†’ Linear(512 â†’ 256)             â”‚  â•‘     â”‚
â”‚   â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â•‘     â”‚
â”‚   â•‘                              â”‚                                    â•‘     â”‚
â”‚   â•‘              (Layer Norm + Residual at each step)                â•‘     â”‚
â”‚   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•     â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚                    OUTPUT PROJECTION                              â”‚     â”‚
â”‚   â”‚                    (256 â†’ vocab_size)                             â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚                     Token Probabilities                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 Transformer Hyperparameters

```python
TRANSFORMER_CONFIG = {
    # Token embedding (extended vocab for conditioning tokens)
    "vocab_size": 36 + 29,  # 36 base + 29 conditioning tokens = 65
    "token_embedding_dim": 256,
    
    # Transformer architecture
    "num_layers": 4,
    "num_heads": 4,
    "d_model": 256,
    "d_ff": 512,  # Feed-forward hidden dimension
    "dropout": 0.1,
    
    # Sequence
    "max_seq_length": 25,  # 20 + up to 5 conditioning prefix tokens
}

# Additional conditioning tokens for Transformer
CONDITIONING_TOKENS = {
    # Genre tokens (IDs 36-44)
    "<ACOUSTIC>": 36, "<BALLAD>": 37, "<BLUES>": 38, "<COUNTRY>": 39,
    "<FOLK>": 40, "<INDIE>": 41, "<JAZZ>": 42, "<POP>": 43, "<ROCK>": 44,
    
    # Emotion tokens (IDs 45-52)
    "<DRAMATIC>": 45, "<ENERGETIC>": 46, "<HOPEFUL>": 47, "<MELANCHOLIC>": 48,
    "<MELLOW>": 49, "<NOSTALGIC>": 50, "<PEACEFUL>": 51, "<UPBEAT>": 52,
    
    # Key tokens (IDs 53-64)
    "<KEY_A>": 53, "<KEY_Am>": 54, "<KEY_Bm>": 55, "<KEY_C>": 56,
    "<KEY_D>": 57, "<KEY_Dm>": 58, "<KEY_E>": 59, "<KEY_Em>": 60,
    "<KEY_F>": 61, "<KEY_Fm>": 62, "<KEY_G>": 63, "<KEY_Gm>": 64,
}
```

---

## 7. Training Specification

### 7.1 Training Configuration

```python
TRAINING_CONFIG = {
    # Data
    "train_samples": 129,
    "val_samples": 27,
    "test_samples": 29,
    "batch_size": 16,
    
    # Optimization
    "optimizer": "AdamW",
    "learning_rate": 5e-4,
    "weight_decay": 0.01,
    "max_epochs": 100,
    
    # Learning rate schedule
    "scheduler": "CosineAnnealingLR",
    "warmup_epochs": 5,
    
    # Early stopping
    "patience": 15,
    "min_delta": 0.001,
    
    # Loss
    "loss_function": "CrossEntropyLoss",
    "label_smoothing": 0.1,
    
    # Regularization
    "dropout": 0.2,  # LSTM
    "gradient_clip": 1.0,
}
```

### 7.2 Training Loop (High-Level)

```
For each epoch:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ TRAINING PHASE                                                  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ For each batch:                                                 â”‚
    â”‚   1. Extract prompt features from batch                         â”‚
    â”‚   2. Get target sequences (with teacher forcing)                â”‚
    â”‚   3. Forward pass â†’ logits                                      â”‚
    â”‚   4. Compute cross-entropy loss                                 â”‚
    â”‚   5. Backward pass                                              â”‚
    â”‚   6. Gradient clipping                                          â”‚
    â”‚   7. Optimizer step                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ VALIDATION PHASE                                                â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ For each batch:                                                 â”‚
    â”‚   1. Forward pass (no gradient)                                 â”‚
    â”‚   2. Compute validation loss                                    â”‚
    â”‚   3. Generate samples (autoregressive)                          â”‚
    â”‚   4. Compute accuracy metrics                                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ CHECKPOINTING                                                   â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ If val_loss improved:                                           â”‚
    â”‚   - Save model checkpoint                                       â”‚
    â”‚   - Reset patience counter                                      â”‚
    â”‚ Else:                                                           â”‚
    â”‚   - Increment patience counter                                  â”‚
    â”‚   - If patience exhausted â†’ stop training                       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.3 Teacher Forcing

During training, we use **teacher forcing** â€” feeding the ground truth tokens as input, rather than the model's own predictions:

```
Without teacher forcing (slow, error propagates):
  <BOS> â†’ model predicts "G" â†’ use "G" as next input â†’ model predicts "E" (wrong!)
                                                        â†’ error propagates...

With teacher forcing (fast, stable):
  <BOS> â†’ model predicts "G" â†’ ignore prediction, use ground truth "G"
  "G"   â†’ model predicts "D" â†’ ignore prediction, use ground truth "D"
  "D"   â†’ model predicts "Em" â†’ ignore prediction, use ground truth "Em"
  ...
  
  All predictions are compared to ground truth in parallel
  Loss = CrossEntropy(predictions, ground_truth)
```

---

## 8. Inference Specification

### 8.1 Autoregressive Generation

During inference, we generate one token at a time:

```
Step 1: Input = [<BOS>]
        Model predicts â†’ distribution over vocab
        Sample or argmax â†’ "G"
        
Step 2: Input = [<BOS>, G]
        Model predicts â†’ distribution over vocab
        Sample or argmax â†’ "D"
        
Step 3: Input = [<BOS>, G, D]
        Model predicts â†’ distribution over vocab
        Sample or argmax â†’ "Em"
        
... continue until <EOS> or max_length ...

Final output: [<BOS>, G, D, Em, C, <SEP>, D, _, D, U, _, D, U, _, <EOS>]
```

### 8.2 Sampling Strategies

```python
INFERENCE_CONFIG = {
    # Greedy decoding (deterministic)
    "greedy": {
        "description": "Always pick highest probability token",
        "use_case": "Reproducible outputs, evaluation"
    },
    
    # Temperature sampling (stochastic)
    "temperature": {
        "temperature": 0.8,  # Lower = more focused, Higher = more random
        "description": "Scale logits before softmax",
        "use_case": "Creative variety"
    },
    
    # Top-k sampling
    "top_k": {
        "k": 10,
        "description": "Sample from top-k most likely tokens",
        "use_case": "Balanced creativity/quality"
    },
    
    # Top-p (nucleus) sampling
    "top_p": {
        "p": 0.9,
        "description": "Sample from smallest set with cumulative prob â‰¥ p",
        "use_case": "Dynamic vocabulary restriction"
    },
}
```

---

## 9. File Structure for Chat 7

```
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tokenizer.py          # NEW: Vocabulary, encode/decode
â”‚   â”œâ”€â”€ feature_encoder.py    # NEW: Prompt features â†’ conditioning vector
â”‚   â”œâ”€â”€ lstm_model.py         # NEW: LSTM sequence model
â”‚   â”œâ”€â”€ transformer_model.py  # NEW: Transformer sequence model
â”‚   â”œâ”€â”€ prompt_parser.py      # EXISTING: Rule-based parser
â”‚   â”œâ”€â”€ neural_parser.py      # EXISTING: DistilBERT parser
â”‚   â””â”€â”€ inference.py          # EXISTING: Will be extended
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py            # NEW: Training loop, checkpointing
â”‚   â”œâ”€â”€ dataset.py            # NEW: PyTorch Dataset class
â”‚   â””â”€â”€ metrics.py            # NEW: Training metrics
â”‚
â””â”€â”€ configs/
    â””â”€â”€ model_config.yaml     # NEW: All hyperparameters

notebooks/
â”œâ”€â”€ 03_train_lstm.ipynb       # NEW: LSTM training notebook
â””â”€â”€ 04_train_transformer.ipynb # NEW: Transformer training notebook
```

---

## 10. Evaluation Plan (Preview for Chat 10)

### 10.1 Metrics to Implement

| Metric | What It Measures | How |
|--------|------------------|-----|
| **Chord Accuracy** | % chords in correct key | Compare to diatonic chords |
| **Progression Validity** | Musical sensibility | Check against known patterns |
| **Strum Pattern Validity** | Correct format | 8 chars, only D/U/_ |
| **Diversity** | Output variety | Unique outputs / total |
| **Perplexity** | Model confidence | exp(avg loss) |

### 10.2 Comparison Framework

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ABLATION STUDY PLAN                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Experiment 1: Rule-based vs LSTM vs Transformer              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚  â€¢ Same test set                                               â”‚
â”‚  â€¢ Same prompt features                                        â”‚
â”‚  â€¢ Compare all metrics                                         â”‚
â”‚                                                                â”‚
â”‚  Experiment 2: Conditioning ablation                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚  â€¢ Full conditioning vs partial vs none                        â”‚
â”‚  â€¢ Which features matter most?                                 â”‚
â”‚                                                                â”‚
â”‚  Experiment 3: Model size ablation                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚  â€¢ LSTM: 1 layer vs 2 layers                                   â”‚
â”‚  â€¢ Transformer: 2 layers vs 4 layers                           â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Summary of Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Models | LSTM + Transformer | Ablation study for thesis |
| Implementation order | LSTM first | Simpler to debug |
| Strumming tokenization | Character-by-character | D, U, _ are atomic |
| Chord tokenization | One token per chord | Chords are atomic |
| Sequence format | Combined with `<SEP>` | Single model |
| Feature encoding | Learned embeddings | Captures relationships |
| LSTM conditioning | Initial hâ‚€ + concatenation | Maximum info flow |
| Transformer conditioning | Prefix tokens | Natural for attention |
| Training | Teacher forcing + early stopping | Stable training |
| Inference | Temperature/top-k sampling | Creative variety |

---

## 12. Next Steps (Chat 7 Preview)

1. **Implement Tokenizer** (`src/models/tokenizer.py`)
2. **Implement Feature Encoder** (`src/models/feature_encoder.py`)
3. **Implement LSTM Model** (`src/models/lstm_model.py`)
4. **Create Training Dataset** (`src/train/dataset.py`)
5. **Implement Training Loop** (`src/train/trainer.py`)
6. **Train LSTM on Colab** (`notebooks/03_train_lstm.ipynb`)
7. **Implement Transformer Model** (`src/models/transformer_model.py`)
8. **Train Transformer** (`notebooks/04_train_transformer.ipynb`)
9. **Compare Results**

---

**Document Complete** âœ“

*This specification will guide all Chat 7 implementation work.*
