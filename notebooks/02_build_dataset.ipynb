{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé∏ Guitar Strum Generator - Dataset Construction\n",
    "\n",
    "**Notebook 02: Build Training Dataset**\n",
    "\n",
    "This notebook creates the training dataset for your thesis by combining:\n",
    "1. **Synthetic samples (70%)** - Generated using the rule-based system\n",
    "2. **Real progressions (30%)** - Sampled from Chordonomicon dataset\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Rohan Rajendra Dhanawade  \n",
    "**Thesis:** A Conversational AI System for Symbolic Guitar Strumming Pattern and Chord Progression Generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "First, let's clone your repository and install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (if running in Colab)\n",
    "import os\n",
    "\n",
    "if not os.path.exists('guitar-strum-gen'):\n",
    "    !git clone https://github.com/rohand575/guitar-strum-gen.git\n",
    "    %cd guitar-strum-gen\n",
    "else:\n",
    "    %cd guitar-strum-gen\n",
    "    !git pull\n",
    "\n",
    "print(\"\\n‚úÖ Repository ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pydantic datasets pandas tqdm\n",
    "\n",
    "# Install the project in development mode\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify imports work\n",
    "from src.data.schema import GuitarSample, VALID_GENRES, VALID_EMOTIONS\n",
    "from src.data.build_dataset import (\n",
    "    DatasetConfig, \n",
    "    build_dataset,\n",
    "    generate_synthetic_sample,\n",
    "    load_chordonomicon_huggingface,\n",
    "    PROMPT_TEMPLATES\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ All imports successful!\")\n",
    "print(f\"   - {len(VALID_GENRES)} genres available\")\n",
    "print(f\"   - {len(VALID_EMOTIONS)} emotions available\")\n",
    "print(f\"   - {len(PROMPT_TEMPLATES)} prompt templates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Chordonomicon Dataset\n",
    "\n",
    "Let's load and explore the Chordonomicon dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chordonomicon from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"üì• Loading Chordonomicon dataset from Hugging Face...\")\n",
    "print(\"   (This may take a few minutes on first run)\\n\")\n",
    "\n",
    "chordonomicon = load_dataset(\"ailsntua/Chordonomicon\", split=\"train\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(chordonomicon):,} chord progressions!\")\n",
    "print(f\"\\nColumns: {chordonomicon.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easier exploration\n",
    "import pandas as pd\n",
    "\n",
    "df = chordonomicon.to_pandas()\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"   Total entries: {len(df):,}\")\n",
    "print(f\"   Memory usage: {df.memory_usage().sum() / 1024**2:.1f} MB\")\n",
    "print(\"\\nüìã First few entries:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few example chord progressions\n",
    "print(\"üéµ Example chord progressions from Chordonomicon:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    row = df.iloc[i]\n",
    "    chords = row['chords'][:200] + \"...\" if len(str(row['chords'])) > 200 else row['chords']\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Genre: {row.get('main_genre', 'N/A')}\")\n",
    "    print(f\"  Chords: {chords}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre distribution in Chordonomicon\n",
    "if 'main_genre' in df.columns:\n",
    "    print(\"üìä Genre distribution in Chordonomicon:\")\n",
    "    genre_counts = df['main_genre'].value_counts().head(15)\n",
    "    for genre, count in genre_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"   {genre:20} {count:>8,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Chord Parsing\n",
    "\n",
    "Let's test our chord parsing functions on real Chordonomicon data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.build_dataset import parse_chordonomicon_chord_string, normalize_chord\n",
    "\n",
    "# Test parsing on a few examples\n",
    "print(\"üîß Testing chord parsing on Chordonomicon samples:\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    row = df.iloc[i * 1000]  # Sample every 1000th row for variety\n",
    "    original = str(row['chords'])[:100]\n",
    "    chords, section = parse_chordonomicon_chord_string(str(row['chords']))\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Original: {original}...\")\n",
    "    print(f\"  Parsed ({len(chords)} chords): {chords[:8]}\")\n",
    "    print(f\"  Section: {section}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Training Dataset\n",
    "\n",
    "Now let's build the complete training dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for dataset generation\n",
    "config = DatasetConfig(\n",
    "    total_samples=250,        # Total samples to generate\n",
    "    synthetic_ratio=0.70,     # 70% synthetic, 30% from Chordonomicon\n",
    "    train_ratio=0.70,         # 70% train, 15% val, 15% test\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"üìä Dataset Configuration:\")\n",
    "print(f\"   Total samples: {config.total_samples}\")\n",
    "print(f\"   Synthetic: {config.num_synthetic} ({config.synthetic_ratio*100:.0f}%)\")\n",
    "print(f\"   From Chordonomicon: {config.num_real} ({(1-config.synthetic_ratio)*100:.0f}%)\")\n",
    "print(f\"   Train/Val/Test: {config.train_ratio*100:.0f}%/{config.val_ratio*100:.0f}%/{config.test_ratio*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset!\n",
    "import random\n",
    "random.seed(config.random_seed)\n",
    "\n",
    "splits = build_dataset(\n",
    "    config=config,\n",
    "    chordonomicon_path=None,  # We'll use Hugging Face\n",
    "    use_huggingface=True,\n",
    "    output_dir=\"data/processed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View generated files\n",
    "!ls -la data/processed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the generated dataset\n",
    "import json\n",
    "\n",
    "# Load stats\n",
    "with open('data/processed/stats.json', 'r') as f:\n",
    "    stats = json.load(f)\n",
    "\n",
    "print(\"üìä Generated Dataset Statistics:\")\n",
    "print(f\"\\n   Total samples: {stats['total']}\")\n",
    "print(f\"   Unique progressions: {stats['unique_progressions']}\")\n",
    "print(f\"   Unique patterns: {stats['unique_patterns']}\")\n",
    "print(f\"   Tempo range: {stats['tempo_range'][0]} - {stats['tempo_range'][1]} BPM\")\n",
    "print(f\"   Average tempo: {stats['avg_tempo']:.1f} BPM\")\n",
    "\n",
    "print(f\"\\nüìÅ Splits:\")\n",
    "for split, count in stats['splits'].items():\n",
    "    print(f\"   {split}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre distribution\n",
    "print(\"üéµ Genre Distribution:\")\n",
    "for genre, count in sorted(stats['genres'].items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = count / stats['total'] * 100\n",
    "    bar = '‚ñà' * int(pct / 2)\n",
    "    print(f\"   {genre:12} {count:3} ({pct:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion distribution\n",
    "print(\"üí≠ Emotion Distribution:\")\n",
    "for emotion, count in sorted(stats['emotions'].items(), key=lambda x: x[1], reverse=True):\n",
    "    pct = count / stats['total'] * 100\n",
    "    bar = '‚ñà' * int(pct / 2)\n",
    "    print(f\"   {emotion:12} {count:3} ({pct:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some sample entries\n",
    "print(\"üìù Sample Entries from Dataset:\\n\")\n",
    "\n",
    "with open('data/processed/dataset.jsonl', 'r') as f:\n",
    "    samples = [json.loads(line) for line in f.readlines()[:10]]\n",
    "\n",
    "for i, sample in enumerate(samples[:5]):\n",
    "    print(f\"--- Sample {i+1} ({sample['id']}) ---\")\n",
    "    print(f\"Prompt: \\\"{sample['prompt']}\\\"\")\n",
    "    print(f\"Chords: {sample['chords']}\")\n",
    "    print(f\"Pattern: {sample['strum_pattern']}\")\n",
    "    print(f\"Key: {sample['key']} {sample['mode']} | Tempo: {sample['tempo']} BPM\")\n",
    "    print(f\"Genre: {sample['genre']} | Emotion: {sample['emotion']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Dataset Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Genre distribution\n",
    "ax1 = axes[0, 0]\n",
    "genres = list(stats['genres'].keys())\n",
    "genre_counts = list(stats['genres'].values())\n",
    "ax1.barh(genres, genre_counts, color='steelblue')\n",
    "ax1.set_xlabel('Count')\n",
    "ax1.set_title('Genre Distribution')\n",
    "\n",
    "# Emotion distribution\n",
    "ax2 = axes[0, 1]\n",
    "emotions = list(stats['emotions'].keys())\n",
    "emotion_counts = list(stats['emotions'].values())\n",
    "ax2.barh(emotions, emotion_counts, color='coral')\n",
    "ax2.set_xlabel('Count')\n",
    "ax2.set_title('Emotion Distribution')\n",
    "\n",
    "# Mode distribution (pie)\n",
    "ax3 = axes[1, 0]\n",
    "modes = list(stats['modes'].keys())\n",
    "mode_counts = list(stats['modes'].values())\n",
    "ax3.pie(mode_counts, labels=modes, autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "ax3.set_title('Mode Distribution (Major vs Minor)')\n",
    "\n",
    "# Key distribution\n",
    "ax4 = axes[1, 1]\n",
    "keys = list(stats['keys'].keys())\n",
    "key_counts = list(stats['keys'].values())\n",
    "ax4.bar(keys, key_counts, color='purple', alpha=0.7)\n",
    "ax4.set_xlabel('Key')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_title('Key Distribution')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/processed/dataset_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualization saved to data/processed/dataset_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Dataset Files\n",
    "\n",
    "Download the generated files to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file for easy download\n",
    "!cd data/processed && zip -r ../../guitar_dataset.zip *.jsonl *.json *.png 2>/dev/null\n",
    "\n",
    "print(\"üì¶ Created guitar_dataset.zip\")\n",
    "print(\"\\nContents:\")\n",
    "!unzip -l guitar_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (only works in Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('guitar_dataset.zip')\n",
    "    print(\"\\n‚úÖ Download started!\")\n",
    "except ImportError:\n",
    "    print(\"\\nüìÅ Not running in Colab. Files are in data/processed/\")\n",
    "    print(\"   You can manually download:\")\n",
    "    print(\"   - train.jsonl\")\n",
    "    print(\"   - val.jsonl\")\n",
    "    print(\"   - test.jsonl\")\n",
    "    print(\"   - stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "‚úÖ **Training dataset** with 250 samples:\n",
    "- 175 synthetic samples (70%) from rule-based system\n",
    "- 75 real progressions (30%) from Chordonomicon\n",
    "\n",
    "‚úÖ **Rich prompt diversity** with 68 unique templates\n",
    "\n",
    "‚úÖ **Balanced distribution** across:\n",
    "- 9 genres (pop, rock, folk, ballad, country, blues, jazz, indie, acoustic)\n",
    "- 8 emotions (upbeat, melancholic, mellow, energetic, peaceful, dramatic, hopeful, nostalgic)\n",
    "- Major and minor keys\n",
    "\n",
    "### Output Files\n",
    "\n",
    "| File | Description |\n",
    "|------|-------------|\n",
    "| `train.jsonl` | Training set (70%) |\n",
    "| `val.jsonl` | Validation set (15%) |\n",
    "| `test.jsonl` | Test set (15%) |\n",
    "| `dataset.jsonl` | Complete dataset |\n",
    "| `stats.json` | Dataset statistics |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **Notebook 03: Training** to train the neural sequence model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Dataset construction complete!\")\n",
    "print(\"\\nüìä Final Stats:\")\n",
    "print(f\"   - {stats['total']} total samples\")\n",
    "print(f\"   - {stats['unique_progressions']} unique chord progressions\")\n",
    "print(f\"   - {stats['unique_patterns']} unique strumming patterns\")\n",
    "print(f\"   - Train/Val/Test: {stats['splits']['train']}/{stats['splits']['val']}/{stats['splits']['test']}\")\n",
    "print(\"\\nüöÄ Ready for neural model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
